---
title: "Forecasting Advance Retail Sales: Retail Trade"
author: "Nathan Kim"
date: "Fall Quarter 2021"
output:
  html_document: default
  pdf_document: default
---

```{r, echo=F, warning=F, message=F}
library(knitr)
opts_chunk$set(tidy.opts=list(width.cutoff=60), tidy = TRUE)
```

```{r, echo=F}
rm(list=ls(all=TRUE))
fred_key = "e36f33d227cf9cca9a962cc847d4e0af"
```

```{r, warning=F, message=F}
library(fredr)
library(fpp2)
library(vars)
library(tseries)
library(forecast)
library(rstan)
library(prophet)
library(ggplot2)
library(MLmetrics)
library(tseries)
library(strucchange)

# Insert own API key from FRED 
fredr_set_key(fred_key)
```

# Data Inspection 

```{r}
sales <- fredr(series_id="RSXFSN")
sales_ts <- ts(log(sales$value), start=1992, frequency=12) 
any(is.na(sales_ts))
```

```{r, warning=F}
adf.test(sales_ts)
```
The ADF test tells us that we fail to reject the null hypothesis of non-stationarity at the 5% significance level. Therefore, we cannot claim that this series is stationary. 

```{r}
autoplot(sales_ts) +
  ylab("Log(Advance Sales)") + 
  ggtitle("Monthly Advance Retail Sales")
autoplot(stl(sales_ts, s.window="periodic"), main="STL Decomposition")
ggtsdisplay(sales_ts, main="Series with ACF and PACF") 
```

After taking the log of the series, the seasonality has become more constant. The decay in spikes in the ACF and the few recurring significant spikes in the PACF suggest that this series follows an S-AR(p) process. Moreover, we see a clear positive trend as well as some cyclical dynamics in the remainder component. 

# Splitting Data
```{r}
n <- round((2/3)*length(sales_ts)) 

# 2/3 train set 
train <- ts(sales_ts[1:n], start=1992, frequency=12) 

# 1/3 test set 
test  <- ts(sales_ts[n+1:length(sales_ts)], start=2012 + (6/12), frequency=12)
test  <- na.remove(test)
```

Here, we split the time series data into training and testing sets to allow for backtesting. 

# Comparing Models

## ARIMA 

### Simple Train-Test Split
```{r}
f_arima <- forecast(auto.arima(train), h=length(sales_ts)-n)

autoplot(train, series="Train") +
  autolayer(test, series="Test") + 
  autolayer(f_arima$mean, series="ARIMA") + 
  ggtitle("Forecasting with ARIMA") + 
  ylab("Log(Sales)") + 
  labs(colour="")
```

The ARIMA model seems to perform well until around 2021 where there is a drastic increase in sales likely caused by the American Rescue Plan Act of 2021. 

### Recursive Backtesting 
```{r}
arima_recur <- c() 

for (i in n:length(sales_ts)) { 
  predict     <- forecast(auto.arima(ts(sales_ts[1:i], frequency=12),
                                     method="CSS"), h=12) 
  actual      <- sales_ts[(i+1):(i+12)] 
  arima_recur <- cbind(arima_recur, MAPE(predict$mean, actual))
}

arima_recur    <- na.omit(as.data.frame(t(arima_recur)))
h              <- c(1:nrow(arima_recur))
arima_recur_df <- cbind.data.frame(h, arima_recur)
```

### Rolling Window Backtesting 
```{r}
arima_roll <- c()

for (i in 0:(length(sales_ts)-n)) { 
  predict    <- forecast(auto.arima(ts(sales_ts[(1+i):(n+i)], frequency=12), 
                                    method="CSS"), h=12) 
  actual     <- sales_ts[(n+i+1):(n+i+12)]
  arima_roll <- cbind(arima_roll, MAPE(predict$mean, actual))
}

arima_roll    <- na.omit(as.data.frame(t(arima_roll)))
h             <- c(1:nrow(arima_roll))
arima_roll_df <- cbind.data.frame(h, arima_roll) 
```

```{r}
mean(arima_recur$V1)
mean(arima_roll$V1)
```

## ETS 

### Simple Train-Test Split 
```{r}
f_ets <- forecast(ets(train), h=length(sales_ts)-n)

autoplot(train, series="Train") +
  autolayer(test, series="Test") + 
  autolayer(f_ets$mean, series="ETS") + 
  ggtitle("Forecasting with ETS") + 
  ylab("Log(Sales)") + 
  labs(colour="")  
```

Here, ETS does not seem to be able to forecast well into long horizons which motivates the use of recursive and rolling window backtesting even more.

### Recursive Backtesting 
```{r}
ets_recur <- c() 

for (i in n:length(sales_ts)) { 
  predict   <- forecast(ets(ts(sales_ts[1:i], frequency=12)), h=12) 
  actual    <- sales_ts[(i+1):(i+12)] 
  ets_recur <- cbind(ets_recur, MAPE(predict$mean, actual))
}

ets_recur    <- na.omit(as.data.frame(t(ets_recur)))
h            <- c(1:nrow(ets_recur))
ets_recur_df <- cbind.data.frame(h, ets_recur) 
```

### Rolling Window Backtesting 
```{r}
ets_roll <- c()

for (i in 0:(length(sales_ts)-n)) { 
  predict  <- forecast(ets(ts(sales_ts[(1+i):(n+i)], frequency=12)), h=12) 
  actual   <- sales_ts[(n+i+1):(n+i+12)]
  ets_roll <- cbind(ets_roll, MAPE(predict$mean, actual))
}

ets_roll    <- na.omit(as.data.frame(t(ets_roll)))
h           <- c(1:nrow(ets_roll))
ets_roll_df <- cbind.data.frame(h, ets_roll) 
```

```{r}
mean(ets_recur$V1)
mean(ets_roll$V1)
```

## Holt-Winters 

### Simple Train-Test Split 
```{r}
f_hw <- forecast(HoltWinters(train), h=length(sales_ts)-n)

autoplot(train, series="Train") + 
  autolayer(test, series="Test") + 
  autolayer(f_hw$mean, series="HW") + 
  labs(colour="") + 
  ggtitle("Forecasting with Holt Winters") +
  ylab("Log(Sales)")
```

Holt Winters seems to provide similar forecasts to the ARIMA model. 

### Recursive Backtesting 
```{r}
hw_recur <- c() 

for (i in n:length(sales_ts)) { 
  predict  <- forecast(hw(ts(sales_ts[1:i], frequency=12)), h=12) 
  actual   <- sales_ts[(i+1):(i+12)] 
  hw_recur <- cbind(hw_recur, MAPE(predict$mean, actual))
}

hw_recur    <- na.omit(as.data.frame(t(hw_recur)))
h           <- c(1:nrow(hw_recur))
hw_recur_df <- cbind.data.frame(h, hw_recur) 
```

### Rolling Window Backtesting 
```{r}
hw_roll <- c()

for (i in 0:(length(sales_ts)-n)) { 
  predict <- forecast(hw(ts(sales_ts[(1+i):(n+i)], frequency=12)), h=12) 
  actual  <- sales_ts[(n+i+1):(n+i+12)]
  hw_roll <- cbind(hw_roll, MAPE(predict$mean, actual))
}

hw_roll    <- na.omit(as.data.frame(t(hw_roll)))
h          <- c(1:nrow(hw_roll))
hw_roll_df <- cbind.data.frame(h, hw_roll) 
```

```{r}
mean(hw_recur$V1)
mean(hw_roll$V1)
```

## Facebook Prophet 

### Simple Train-Test Split 
```{r, message=F}
sales_df           <- cbind.data.frame(sales$date, log(sales$value))
colnames(sales_df) <- c("ds", "y")

train_prophet <- sales_df[1:n,]
test_prophet  <- sales_df[n+1:nrow(sales_df),]

prophet      <- prophet(train_prophet)
future       <- make_future_dataframe(prophet, periods=nrow(sales_df)-n, 
                                      freq='month')
f_prophet    <- predict(prophet, future)
f_prophet_ts <- na.remove(ts(f_prophet$yhat[n+1:nrow(f_prophet)], 
                             start=2012 + (6/12), frequency=12))

autoplot(train, series="Train") + 
  autolayer(test, series="Test") + 
  autolayer(f_prophet_ts, series="FBP") + 
  ggtitle("Forecasting with Facebook Prophet") + 
  ylab("Log(Sales)") + 
  labs(colour="")
```

Facebook Prophet provides forecasts that underestimate the test data points. 

### Recursive Backtesting 
```{r, message=F}
fbp_recur <- c() 
for (i in n:length(sales_ts)) { 
  df        <- sales_df[1:i,] 
  model     <- prophet(df)
  future    <- make_future_dataframe(model, periods=12, freq='month')
  f_prophet <- predict(model, future)
  predict   <- f_prophet$yhat[(nrow(f_prophet)-11):nrow(f_prophet)]
  actual    <- sales_ts[(i+1):(i+12)] 
  fbp_recur <- cbind(fbp_recur, MAPE(predict, actual))
}

fbp_recur    <- na.omit(as.data.frame(t(fbp_recur)))
h            <- c(1:nrow(fbp_recur))
fbp_recur_df <- cbind.data.frame(h, fbp_recur) 
```

### Rolling Window Backtesting 
```{r, message=F}
fbp_roll <- c()
for (i in 0:(length(sales_ts)-n)) { 
  df        <- sales_df[(1+i):(n+i),] 
  model     <- prophet(df)
  future    <- make_future_dataframe(model, periods=12, freq='month')
  f_prophet <- predict(model, future)
  predict   <- f_prophet$yhat[(nrow(f_prophet)-11):nrow(f_prophet)]
  actual    <- sales_ts[(n+i+1):(n+i+12)]
  fbp_roll  <- cbind(fbp_roll, MAPE(predict, actual))
}

fbp_roll    <- na.omit(as.data.frame(t(fbp_roll)))
h           <- c(1:nrow(fbp_roll))
fbp_roll_df <- cbind.data.frame(h, fbp_roll) 
```

```{r}
mean(fbp_recur$V1)
mean(fbp_roll$V1)
```

# Combined Forecast 

We will now combine the forecasts by taking the average of the results from the four models above. 

### Simple Train-Test Split 
```{r, message=F, warning=F}
prophet   <- prophet(train_prophet)
future    <- make_future_dataframe(prophet, periods=nrow(sales_df)-n, 
                                   freq='month')
f_prophet <- predict(prophet, future)
fbp       <- ts(na.remove(f_prophet$yhat[n+1:nrow(f_prophet)]), start=2012 + (6/12),
                frequency=12)

arima  <- forecast(auto.arima(train), h=length(sales_ts)-n)
ets    <- forecast(ets(train), h=length(sales_ts)-n)
holtw  <- forecast(HoltWinters(train), h=length(sales_ts)-n)
comb   <- (fbp + arima[["mean"]] + ets[["mean"]] + holtw[["mean"]])/4
```

```{r, message=F, warning=F}
autoplot(sales_ts) + 
  autolayer(arima$mean, series="ARIMA") + 
  autolayer(ets$mean, series="ETS") + 
  autolayer(holtw$mean, series="HW") +
  autolayer(fbp, series="FBP") +
  autolayer(comb, series="Combined") +
  ylab("Log(Sales)") + 
  labs(color="Forecasts") +
  ggtitle("Forecasting Using Combined Model") + 
  xlim(2008,2022) + 
  ylim(12.4, 13.4)
```

### Recursive Backtesting 
```{r, message=F}
comb_recur <- c() 
for (i in n:length(sales_ts)) {
  df1 <- ts(sales_ts[1:i], frequency=12)
  f1  <- forecast(auto.arima(df1, method="CSS"), h=12)
  f2  <- forecast(ets(df1), h=12)
  f3  <- forecast(hw(df1), h=12)
  
  df2       <- sales_df[1:i,]
  m4        <- prophet(df2)
  future    <- make_future_dataframe(m4, periods=12, freq='month')
  f_prophet <- predict(m4, future)
  f4        <- f_prophet$yhat[(nrow(f_prophet)-11):nrow(f_prophet)]
  
  f1 <- as.vector(f1$mean)
  f2 <- as.vector(f2$mean)
  f3 <- as.vector(f3$mean)
  f4 <- as.vector(f4)
  
  predict <- (f1 + f2 + f3 + f4)/4
  actual  <- sales_ts[(i+1):(i+12)] 
  
  comb_recur <- cbind(comb_recur, MAPE(predict, actual))
}

comb_recur    <- na.omit(as.data.frame(t(comb_recur)))
h             <- c(1:nrow(comb_recur))
comb_recur_df <- cbind.data.frame(h, comb_recur) 
```

### Rolling Window Backtesting 
```{r, message=F}

comb_roll <- c()
for (i in 0:(length(sales_ts)-n)) { 
  df1     <- ts(sales_ts[(1+i):(n+i)], frequency=12)
  f1      <- forecast(auto.arima(df1, method="CSS"), h=12)
  f2      <- forecast(ets(df1), h=12)
  f3      <- forecast(hw(df1), h=12)
  
  df2       <- sales_df[(1+i):(n+i),]
  m4        <- prophet(df2)
  future    <- make_future_dataframe(m4, periods=12, freq='month')
  f_prophet <- predict(m4, future)
  f4        <- f_prophet$yhat[(nrow(f_prophet)-11):nrow(f_prophet)]
  
  f1 <- as.vector(f1$mean)
  f2 <- as.vector(f2$mean)
  f3 <- as.vector(f3$mean)
  f4 <- as.vector(f4)
  
  predict   <- (f1 + f2 + f3 + f4)/4
  actual    <- sales_ts[(n+i+1):(n+i+12)] 
  comb_roll <- cbind(comb_roll, MAPE(predict, actual))
 
}

comb_roll    <- na.omit(as.data.frame(t(comb_roll)))
h            <- c(1:nrow(comb_roll))
comb_roll_df <- cbind.data.frame(h, comb_roll)
```

```{r}
mean(comb_recur$V1)
mean(comb_roll$V1)
```

On average, we can see that the combined model performs the best when using both recursive and rolling window backtesting. 

# Results of Backtesting

```{r}
mape1 <- cbind.data.frame(h, arima_recur$V1, ets_recur$V1, 
                         hw_recur$V1, fbp_recur$V1, comb_recur$V1)
colnames(mape1) <- c("Iteration", "ARIMA", "ETS", "HW", "FBP", "Combined")

ggplot() + 
  geom_line(mape1, mapping=aes(x=Iteration, y=ARIMA, color="ARIMA")) + 
  geom_line(mape1, mapping=aes(x=h, y=ETS, color="ETS")) + 
  geom_line(mape1, mapping=aes(x=h, y=HW, color="HW")) + 
  geom_line(mape1, mapping=aes(x=h, y=FBP, color="FBP")) + 
  geom_line(mape1, mapping=aes(x=h, y=Combined, color="Combined")) +
  ggtitle("Recursive Backtesting, 12 Steps Ahead") + 
  ylab("MAPE") + 
  labs(colour="Model")
```

```{r}
mape2 <- cbind.data.frame(h, arima_roll$V1, ets_roll$V1, 
                          hw_roll$V1, fbp_roll$V1, comb_roll$V1)
colnames(mape2) <- c("Iteration", "ARIMA", "ETS", "HW", "FBP", "Combined")

ggplot() + 
  geom_line(mape2, mapping=aes(x=Iteration, y=ARIMA, color="ARIMA")) + 
  geom_line(mape2, mapping=aes(x=h, y=ETS, color="ETS")) + 
  geom_line(mape2, mapping=aes(x=h, y=HW, color="HW")) + 
  geom_line(mape2, mapping=aes(x=h, y=FBP, color="FBP")) + 
  geom_line(mape2, mapping=aes(x=h, y=Combined, color="Combined")) +
  ggtitle("Rolling Window Backtesting, 12 Steps Ahead") + 
  ylab("MAPE") + 
  labs(colour="Model")
```

```{r, message=F}
model  <- prophet(sales_df)
future <- make_future_dataframe(model, periods=12, 
                                   freq='month')

f_prophet <- predict(model, future)

fbp <- ts(na.remove(f_prophet$yhat[1:(nrow(f_prophet)-12)]), 
          start=1992, frequency=12)
ari <- auto.arima(sales_ts)
ets <- ets(sales_ts)
how <- hw(sales_ts)
com <- (ari[["fitted"]] + ets[["fitted"]] + how[["fitted"]] + fbp) / 4
```

```{r}
res <- sales_ts - com
ggtsdisplay(res, main="Residuals from Combined Model")
```

As you can see, the combined model struggles (as with the other models), to capture the spike in sales in 2021. The ACF and PACF plots of the residuals also show that there is still a small amount of signal that the model could not capture. 

```{r}
Box.test(res, type="Box-Pierce")
Box.test(res, type="Ljung-Box")
```

We can see that, statistically, the residuals from the combined model are white noise. This indicates that the combined model is a suitable fit to the data. 

```{r}
cusum   <- efp(res ~ 1, type="Rec-CUSUM")
bound_u <- boundary(cusum, alpha=0.05)
bound_l <- -1*boundary(cusum, alpha=0.05)
horizon <- ts(rep(0, times=length(cusum$process)), start=1992, frequency=12)

autoplot(cusum$process) + 
  autolayer(bound_u, color="red") + 
  autolayer(bound_l, color="red") + 
  autolayer(horizon, color="black") +
  ggtitle("Recursive CUSUM Plot") + 
  ylab("Emperical Fluctuation Process") 

```

The CUSUM plot above shows that our model's structural integrity is not compromised although there are slight dips during major recessions in 2008 and 2019. The model is able to somewhat recover after these dips. 

```{r}
rr <- recresid(res ~ 1)
x  <- c(1:length(rr))
ggplot() + 
  geom_line(cbind.data.frame(x, rr), mapping=aes(x=x, y=rr)) +
  ylab("Recursive Residuals") + 
  xlab("")
```

Finally, our recursive residuals from the combined model seem well behaved with the exception of the final portion of the time series data which is due to the drastic change in sales in the beginning of 2021. 

